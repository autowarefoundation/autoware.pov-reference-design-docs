{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#about-reference-design-guideline-for-pov-vehicles","title":"About Reference Design Guideline for PoV Vehicles","text":"<p>This document serves a guideline to design and deploy a TRL-6 privately-owned vehicles based on Autoware. The readers can take this document as a starting point to select and configure the hardware and software components of the vehicles.</p> <p>The Autoware uses the end-to-end foundation model on the privately-owned vehicles, shown as below.</p> <p></p>"},{"location":"#reference-design-guideline-for-pov-vehicles-documentation-structure","title":"Reference Design Guideline for PoV Vehicles documentation structure","text":"<p>The document publishes the guidelines for Privately-owned Vehicles (PoV), using the following document structure shown below.</p> <p></p> <p>The document consists of four major components:</p> <ul> <li>ODD Definition defines the operation environment of the PoV.</li> <li>Hardware configuration describes the sensors and actuators used on the PoV. There is no reference physical chassis.</li> <li>Software configuration describes the process of deploying the software on ECUs.</li> <li>Evaluation and testing describes the process of evaluating and testing the software for the PoVs. The dataset and performance metrics are shown.</li> </ul> <p>There are fours phases on designing the software for PoV. The details of the roadmap are discussed on this page. Below summarizes the configuration and expected outcomes for each phase. This document provides the design guideline for Vision Pilot in Phase 1.</p> <ul> <li>Vision Pilot (Phase 1): using 1x forward front-facing RGB camera to enable near and long-range sensing to enable SAE Level 2+/2++ automation.</li> <li>Vision Pilot - Plus (Phase 2): using 1x forward front-facing RGB camera and 1x forward front-facing high resolution 4D radar, and 2x Blindspot monitoring automotive corner RADAR  to enable SAE Level 3 automation.</li> <li>Vision Pilot - Pro (Phase 3): using 3x forward front-facing RGB camera, 1x forward front-facing high resolution 4D radar, 2x Blindspot monitoring automotive corner RADAR, and standard 2D Navigational map with GPS to enable SAE Level 3 automation.</li> <li>Vision Drive (Phase 4): using 3x forward front-facing RGB camera, 1x forward front-facing high resolution 4D radar, 2x Blindspot monitoring automotive corner RADAR, and standard 2D Navigational map with GPS to enable SAE Level 4 automation.</li> </ul> <p></p> <p>For more details about the reference design WG, its goals and details of the Autoware Foundation working groups that oversees the project, refer to the Reference Design WG wiki</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>System Configuration</li> <li>ODD</li> <li>Hardware Configuration</li> <li>Software Configuration</li> <li>Evaluation and Testing</li> </ul>"},{"location":"evaluation-and-testing/","title":"Evaluation and Testing","text":"<p>(To be completed)</p>"},{"location":"evaluation-and-testing/#model-evaluation","title":"Model Evaluation","text":"<ul> <li>SceneSeg: https://github.com/autowarefoundation/autoware.privately-owned-vehicles/tree/main/SceneSeg#performance-results</li> <li>DomainSeg: https://github.com/autowarefoundation/autoware.privately-owned-vehicles/tree/main/DomainSeg#performance-results</li> </ul>"},{"location":"evaluation-and-testing/#system-evaluation","title":"System Evaluation","text":"<ul> <li>The overall performance for the whole pipeline.</li> </ul>"},{"location":"hardware-configuration/","title":"Hardware Configuration","text":"<p>(To be completed)</p> <p>This section describes the hardware configurations for PoV vehicles, including</p> <ul> <li>Sensors and Actuators: describes the sensors and actuators used in the reference design.</li> <li>ECUs: describes the ECUs used in the reference design.</li> </ul> <p>The sensors requirements are different based on different PoV version.</p> <ul> <li> <p>Vision Pilot:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>One front-facing 4D RADAR.</li> </ul> </li> <li> <p>Vision Pilot Plus:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> </ul> </li> <li> <p>Vision Pilot Pro:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> <li>Four side camera: 8MP RGB camera with 100 degree horizontal FoV</li> <li>Sat-Nav 2D map</li> </ul> </li> <li> <p>Vision Drive:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>One LWIR Camera: 60 degree horizontal FoV</li> <li>One SWIR Camera: 60 degree horizontal FoV</li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> <li>Four side camera: 8MP RGB camera with 100 degree horizontal FoV</li> <li>Sat-Nav 2D map</li> </ul> </li> </ul> <p>The locations of the sensors are shown in the figure below.</p> <p></p>"},{"location":"hardware-configuration/ECUs/","title":"ECUs","text":"<p>The page lists the ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <ul> <li>ARM-based ECUs: describes the hardware configuration for ARM-based ECUs.</li> <li>x86_64-based ECUs: describes the hardware configuration for x86_64-based ECUs</li> </ul>"},{"location":"hardware-configuration/ECUs/ARM/","title":"ARM-based ECUs","text":"<p>The page lists the ARM-based ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <p>Another page on Autoware Document lists the ECUs being for other use scenarios.</p> <p>(Candidates of the ECUs to be used by the PoV in the alphabetical order)</p>"},{"location":"hardware-configuration/ECUs/x86_64/","title":"X86-based ECUs","text":"<p>The page lists the X86-bsaed ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <p>Another page on Autoware Document lists the ECUs being for other use scenarios.</p> <p>(Candidates of the ECUs to be used by the LSA in the alphabetical order)</p>"},{"location":"hardware-configuration/Sensors-and-Actuators/","title":"Hardware Configuration","text":"<p>(To be completed)</p> <p>The page lists the sensors and actuators being tested for PoV, which can support the ODD of PoV.</p> <p>TODO: Add specific models Reference: https://autowarefoundation.github.io/LSA-reference-design-docs/main/hardware-configuration/Sensors-and-Actuators/</p> <ul> <li>Camera: Sony</li> <li>Radar: Bosch, Continental</li> <li>4D Radar: Bosch, Continental</li> <li>Infrared camera: Sony SenSWIR, Teledyne FLIR</li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot","title":"Vision Pilot","text":"<ul> <li>Camera:<ul> <li>1 forward front-facing 8MP RGB camera with 120 degree horizontal field-of-view.</li> </ul> </li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot-plus","title":"Vision Pilot Plus","text":"<ul> <li>Camera:<ul> <li>1 forward front-facing 8MP RGB camera with 120 degree horizontal field-of-view.</li> </ul> </li> <li>4D Radar:<ul> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> </ul> </li> <li>Radar<ul> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> </ul> </li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot-pro","title":"Vision Pilot Pro","text":"<ul> <li>Camera:<ul> <li>3x 8MP RGB camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> </ul> </li> <li>4D Radar:<ul> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> </ul> </li> <li>Radar<ul> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> <li>Standard 2D Navigational map with GPS</li> </ul> </li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-drive","title":"Vision Drive","text":"<ul> <li>Camera:<ul> <li>3x 8MP RGB camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> </ul> </li> <li>4D Radar:<ul> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> </ul> </li> <li>Radar<ul> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> <li>Standard 2D Navigational map with GPS</li> </ul> </li> <li>Infrared camera<ul> <li>1x forward front-facing 8MP Short Wave Infrared camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> <li>1x forward front-facing 8MP Long Wave Infrared camera with 120 degree horizontal field-of-view.</li> </ul> </li> </ul>"},{"location":"odd-definition/","title":"ODD Definition","text":"<p>Operation Domain Definition (ODD) describes the use scenarios for the vehicles to be designed and deployed. The deployed system should meet the ODD to be functionality completed. This section describes the ODD user scenarios for PoV.</p>"},{"location":"odd-definition/#vision-pilot","title":"Vision Pilot","text":"<p>Vision Pilot aims on Level 2+/2++ on the single lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. A human driver is required to monitor and supervise the system at all times.</p> <p>It will support the following features:</p> <ul> <li>Forward collision warning</li> <li>Lane departure warning</li> <li>Adaptive headlights</li> <li>Automatic Emergency braking</li> <li>Lane departure avoidance</li> <li>Intelligent speed assist</li> <li>Traffic aware autonomous cruise control</li> <li>Autonomous lane keep and road following</li> <li>Hands free autopilot</li> </ul> <p></p>"},{"location":"odd-definition/#vision-pilot-plus","title":"Vision Pilot - Plus","text":"<p>Vision Pilot - Plus aims on Level 3 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"odd-definition/#vision-pilot-pro","title":"Vision Pilot - Pro","text":"<p>Vision Pilot - Pro aims on Level 3 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"odd-definition/#vision-drive","title":"Vision Drive","text":"<p>Vision Drive aims on Level 4 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"software-configuration/","title":"Software Configuration","text":"<p>The software we deploy is VisionPilot</p> <p>AutoSeg is an AI Foundation Model which provides real-time visual scene perception for autonomous vehicles. It utilizes a single neural network backbone to extract diverse image features, a set of context blocks which focus the network's attention on key visual elements within input images, a set of feature necks which aggregate and fuse multi-scale image features, and multiple segmentation and detection heads which provide useful perceptual outputs for autonomous decision making. Overall, the network is split into three branches, a Depth Branch which calculates the per-pixel scene depth, a Scene Branch which focuses on per-pixel scene segmentation, and a Path Branch, which focuses on driving corridor detection through multiple means.</p> <p>AutoSeg is comprised of 6 neural network export models which include:</p> <ul> <li>Scene3D: Metric and Relative depth estimation of scene elements.</li> <li>SceneSeg: Semantic segmentation of all foreground objects.</li> <li>DomainSeg: Semantic segmentation of roadwork zones and construction objects.</li> <li>EgoSpace: Semantic Segmentation of the drivable road surface.</li> <li>EgoPath: End-to-end drivable path prediction on roads, with and without lane markings.</li> <li>EgoLanes: Detection of lanes and road edges defining the driving corridor.</li> </ul> <p></p> <p>To run the VisionPilot, we need to install some prerequisites first.</p> <ul> <li>installation</li> </ul> <p>Now that the Vision Pilot support two kinds of pipeline. You can follow the tutorial below to run the software.</p> <ul> <li>ROS 2</li> <li>Zenoh</li> </ul>"},{"location":"software-configuration/installation/","title":"Software installation","text":"<p>We suggest to use Ubuntu 22.04 for the software stack. Since VisionPilot requires AI packages, we need to install the following packages first.</p>"},{"location":"software-configuration/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>CUDA: Optional for GPU processing.</li> <li>OpenCV: For image and video processing.<ul> <li>Ubuntu: <code>sudo apt install libopencv-dev</code></li> </ul> </li> <li>ONNX Runtime: For model inference.<ul> <li>Download from the GitHub release</li> </ul> </li> <li>LibTorch: For tensor manipulation capabilities.<ul> <li>Download from the PyTorch website</li> </ul> </li> </ul>"},{"location":"software-configuration/installation/#models","title":"Models","text":"<p>The following models will be used in the pipeline.</p> <ul> <li>SceneSeg - segmentation of foreground objects:<ul> <li>Link to Download Pytorch Model Weights *.pth</li> <li>Link to Download Traced Pytorch Model *.pt</li> <li>Link to Download ONNX FP32 Weights *.onnx</li> </ul> </li> <li>Scene3D - monocular depth estimation<ul> <li>Link to Download Pytorch Model Weights *.pth</li> <li>Link to Download Traced Pytorch Model *.pt</li> <li>Link to Download ONNX FP32 Weights *.onnx</li> </ul> </li> <li>DomainSeg - segmentation of construction objects<ul> <li>Link to Download Pytorch Model Weights *.pth</li> <li>Link to Download Traced Pytorch Model *.pt</li> <li>Link to Download ONNX FP32 Weights *.onnx</li> </ul> </li> </ul>"},{"location":"software-configuration/ros2/","title":"ROS 2","text":""},{"location":"software-configuration/ros2/#installation","title":"Installation","text":"<p>We can follow the tutorial to install ROS 2 humble.</p>"},{"location":"software-configuration/ros2/#build","title":"Build","text":"<p>TODO</p>"},{"location":"software-configuration/ros2/#usage","title":"Usage","text":"<p>TODO</p>"},{"location":"software-configuration/zenoh/","title":"Zenoh","text":""},{"location":"software-configuration/zenoh/#installation","title":"Installation","text":"<p>We need install zenoh and some dependencies first.</p> <ul> <li> <p>Zenoh C library: Required for the transportation.</p> <ul> <li>Download from the GitHub release</li> <li>You can also add the Eclipse repository for apt server.</li> </ul> <pre><code>curl -L https://download.eclipse.org/zenoh/debian-repo/zenoh-public-key | sudo gpg --dearmor --yes --output /etc/apt/keyrings/zenoh-public-key.gpg\necho \"deb [signed-by=/etc/apt/keyrings/zenoh-public-key.gpg] https://download.eclipse.org/zenoh/debian-repo/ /\" | sudo tee /etc/apt/sources.list.d/zenoh.list &gt; /dev/null\nsudo apt update\nsudo apt install libzenohc-dev\n</code></pre> </li> <li> <p>CLI11: Used for the command line interface.</p> <ul> <li>Ubuntu: <code>sudo apt install libcli11-dev</code></li> </ul> </li> </ul>"},{"location":"software-configuration/zenoh/#build","title":"Build","text":"<ul> <li>Environment setup</li> </ul> <pre><code>export PATH=/usr/local/cuda/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n</code></pre> <ul> <li>Configure with cmake</li> </ul> <pre><code>mkdir build &amp;&amp; cd build\ncmake .. \\\n-DLIBTORCH_INSTALL_ROOT=/path/to/libtorch/ \\\n-DONNXRUNTIME_ROOTDIR=/path/to/onnxruntime-linux-x64-gpu-1.22.0 \\\n-DUSE_CUDA_BACKEND=True\n</code></pre> <ul> <li>Build</li> </ul> <pre><code>make\n</code></pre>"},{"location":"software-configuration/zenoh/#usage","title":"Usage","text":"<p>After a successful build, you will find two executables in the <code>build</code> directory.</p>"},{"location":"software-configuration/zenoh/#video-visualization","title":"Video Visualization","text":"<p>Subscribe a video from a Zenoh publisher and then publish it to a Zenoh Subscriber.</p> <ul> <li>Usage the video publisher and subscriber</li> </ul> <pre><code># Terminal 1\n./video_publisher -k video/input\n# Terminal 2\n./run_model SceneSeg_FP32.onnx -i video/input -o video/output\n./run_model DomainSeg_FP32.onnx -i video/input -o video/output\n# Terminal 3\n./video_subscriber -k video/output\n</code></pre>"},{"location":"system-configuration/","title":"System Configuration","text":"<p>(To be completed)</p> <p>The section describes the system configuration of the PoV vehicles. It shows the options to design your own PoV vehicles. The configuration consists of ECU selection, development approach, middleware candidates, etc.</p>"},{"location":"system-configuration/#version","title":"Version","text":"<p>There are four phases on the roadmap.</p> <p></p>"},{"location":"system-configuration/#hardware-selection","title":"Hardware Selection","text":"<p>The PoV software stack heavily relies on the inference models, which means high performant GPU is required.</p> <p>Here is the rough GPU requirements:</p> <ul> <li>Vision Pilot: 50-80 TOPS</li> <li>Vision Pilot Plus: 150 TOPS</li> <li>Vision Pilot Pro: 500 TOPS</li> <li>Vision Drive: 1000 TOPS</li> </ul> <p>NVIDIA GPU is well supported. AMD, Qualcomm and other vendors (e.g. NPU Halio-8) are on the roadmap.</p>"},{"location":"system-configuration/#development-approach","title":"Development Approach","text":"<ul> <li>Native Installation: Direct installation on hardware for maximum performance</li> <li>Containerized Development: Reproducible environments with easier team collaboration (on the roadmap)</li> </ul>"},{"location":"system-configuration/#middleware-choice","title":"Middleware Choice","text":"<p>Now the PoV supports two kinds of pipeline:</p> <ul> <li>ROS: Able to bridge with the existing Autoware system.</li> <li>Zenoh: High performant middleware.</li> </ul>"}]}