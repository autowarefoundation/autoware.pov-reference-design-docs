{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#about-reference-design-guideline-for-pov-vehicles","title":"About Reference Design Guideline for PoV Vehicles","text":"<p>This document serves a guideline to design and deploy a TRL-6 privately-owned vehicles based on Autoware. The readers can take this document as a starting point to select and configure the hardware and software components of the vehicles.</p> <p>The Autoware uses the end-to-end foundation model on the privately-owned vehicles, shown as below.</p> <p></p>"},{"location":"#reference-design-guideline-for-pov-vehicles-documentation-structure","title":"Reference Design Guideline for PoV Vehicles documentation structure","text":"<p>The document publishes the guidelines for Privately-owned Vehicles (PoV), using the following document structure shown below.</p> <p></p> <p>The document consists of four major components:</p> <ul> <li>ODD Definition defines the operation environment of the PoV.</li> <li>Hardware configuration describes the sensors and actuators used on the PoV. There is no reference physical chassis.</li> <li>Software configuration describes the process of deploying the software on ECUs.</li> <li>Evaluation and testing describes the process of evaluating and testing the software for the PoVs. The dataset and performance metrics are shown.</li> </ul> <p>There are fours phases on designing the software for PoV. The details of the roadmap are discussed on this page. Below summarizes the configuration and expected outcomes for each phase. This document provides the design guideline for Vision Pilot in Phase 1.</p> <ul> <li>Vision Pilot (Phase 1): using 1x forward front-facing RGB camera to enable near and long-range sensing to enable SAE Level 2+/2++ automation.</li> <li>Vision Pilot - Plus (Phase 2): using 1x forward front-facing RGB camera and 1x forward front-facing high resolution 4D radar, and 2x Blindspot monitoring automotive corner RADAR  to enable SAE Level 3 automation.</li> <li>Vision Pilot - Pro (Phase 3): using 3x forward front-facing RGB camera, 1x forward front-facing high resolution 4D radar, 2x Blindspot monitoring automotive corner RADAR, and standard 2D Navigational map with GPS to enable SAE Level 3 automation.</li> <li>Vision Drive (Phase 4): using 3x forward front-facing RGB camera, 1x forward front-facing high resolution 4D radar, 2x Blindspot monitoring automotive corner RADAR, and standard 2D Navigational map with GPS to enable SAE Level 4 automation.</li> </ul> <p></p> <p>For more details about the reference design WG, its goals and details of the Autoware Foundation working groups that oversees the project, refer to the Reference Design WG wiki</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>System Configuration</li> <li>ODD</li> <li>Hardware Configuration</li> <li>Software Configuration</li> <li>Evaluation and Testing</li> </ul>"},{"location":"evaluation-and-testing/","title":"Evaluation and Testing","text":"<p>The evaluation for each model can be found on the GitHub repo of the PoV. </p> <ul> <li> <p>SceneSeg: link</p> </li> <li> <p>DomainSeg: link</p> </li> <li> <p>Scene3D: link</p> </li> <li> <p>EgoPath: link</p> </li> </ul>"},{"location":"evaluation-and-testing/#system-evaluation-results","title":"System Evaluation Results","text":"<p>The section shows the benchmark results of the VisionPilot model on different hardware environment as the references. There are two procedures to conduct the benchmark:</p> <ul> <li> <p>Just-based: link</p> </li> <li> <p>Make-base: link</p> </li> </ul> <p>Two sets of computation configurations are used to benchmark the pipile:</p> <ul> <li>X86-based Computer: link</li> <li>ARM-based Computer: link</li> </ul>"},{"location":"evaluation-and-testing/#adlink-ava-3510","title":"ADLINK AVA-3510","text":""},{"location":"evaluation-and-testing/#hardware-spec","title":"Hardware Spec","text":"<ul> <li>CPU:  Intel Xeon E-2278GE (16 cores)</li> <li>GPU: NVIDIA Quadro RTX 5000.</li> <li>Memory: 64GB LPDDR5/16G on RTX 5000</li> <li>Driver: Driver Version: 580.82.07 &amp; CUDA Version: 13.0</li> <li>ROS: ROS Jazzy &amp; Zenoh</li> <li>Runtime: TensorRT</li> <li>OS: Ubuntu 24.04.3</li> </ul> <p>link to 3510 (discontinued)</p> <p>link to AL30 ( Autonomous Driving Solutions)</p>"},{"location":"evaluation-and-testing/#benchmark-result","title":"Benchmark Result","text":"<ul> <li> <p>Zenoh:</p> Model CPU Utilization GPU Utilization Peak Memory Usage Frame Rate SceneSeg 13% 65% 20G 58 DomainSeg 14% 62% 21G 58 Scene3D 15% 69% 19G 57 EgoSpace <ul> <li>SceneSeg</li> </ul> <pre><code>Current FPS: 58\n--- Per-frame Timing (microseconds) ---\n* Total processing time: 17229 us\n* Preprocessing time: 1200 us\n* Inference time: 15829 us      \n* Output time: 199 us\n</code></pre> <ul> <li>DomainSeg</li> </ul> <pre><code>Current FPS: 58\n--- Per-frame Timing (microseconds) ---\n* Total processing time: 16625 us\n* Preprocessing time: 1203 us\n* Inference time: 15250 us\n* Output time: 171 us\n</code></pre> <ul> <li>Scene3D</li> </ul> <pre><code>Current FPS: 57\n--- Per-frame Timing (microseconds) ---\n* Total processing time: 17484 us\n* Preprocessing time: 204 us\n* Inference time: 16817 us\n* Output time: 462 us\n</code></pre> </li> <li> <p>ROS 2:</p> Model CPU Utilization GPU Utilization Peak Memory Usage Frame Rate SceneSeg 17% 72% 6G 60 DomainSeg 16% 68% 6G 60 Scene3D 16% 67% 5.9G 60 EgoSpace <ul> <li>SceneSeg</li> </ul> <pre><code>* Current FPS: 60\n--- Per-frame Timing (microseconds) --- \n* Total processing time: 16272 us\n* Preprocessing time: 178 us\n* Inference time: 15713 us\n* Output time: 381 us\n</code></pre> <ul> <li>DomainSeg</li> </ul> <pre><code>* Current FPS: 60.00\n--- Per-frame Timing (microseconds) --- \n* Total processing time: 15393 us\n* Preprocessing time: 253 us\n* Inference time: 14672 us\n* Output time: 467 us\n-------------------------- \n</code></pre> </li> </ul>"},{"location":"evaluation-and-testing/#arm-processors-and-nvidia-agx-orin","title":"ARM processors and nVidia AGX Orin","text":""},{"location":"evaluation-and-testing/#hardware-spec_1","title":"Hardware spec:","text":"<ul> <li>CPU: 12-core ARM Cortex-A78AE CPU at 2.2GHz.</li> <li>GPU: NVIDIA Ampere GPU with 2048 CUDA Cores.</li> <li>Memory: 64GB LPDDR5. The system and GPU memories are shared.</li> <li>Driver: The NVIDIA JetPack 6.0 (Ubuntu 22.04 LTS based) was used.</li> <li>ROS: ROS Humble with Autoware recommended Cyclone DDS settings.</li> <li>Runtime: ONNX runtime  1.19.0 or TensorRT</li> </ul> <p>link to nVidia Jetson Orin AGX</p>"},{"location":"evaluation-and-testing/#benchmark-results","title":"Benchmark results:","text":"Model CPU Utilization GPU Utilization Peak Memory Usage Frame Rate SceneSeg   (ONNX runtime) 91%  ~ 99% 99% 45G  including network model (~30G) + other process (15G) 8 SceneSeg   (TensorRT runtime - FP16) 57 ~ 66 % 74 % 0.8 % (~0.50 GB) 29.12 DomainSeg  (TensorRT runtime - FP16) 56 ~ 60 % 88 % 0.8 % (~0.50 GB) 29.85 Scene3D  (TensorRT runtime - FP16) 53 ~ 56 % 82 % 0.6 % (~0.38 GB) 29.90 SceneSeg  (TensorRT runtime - FP32) 42 ~ 49 % 99 % 0.6 % (~0.38 GB) 17.10 DomainSeg  (TensorRT runtime - FP32) 43 ~ 47 % 99 % 0.6 % (~0.38 GB) 17.07 Scene3D  (TensorRT runtime - FP32) 44 ~ 46 % 99 % 0.6 % (~0.38 GB) 17.03 <p>link to the instructions and complete results.</p> <ul> <li>Demo Video: link</li> </ul>"},{"location":"hardware-configuration/","title":"Hardware Configuration","text":"<p>(To be completed)</p> <p>This section describes the hardware configurations for PoV vehicles, including</p> <ul> <li>Sensors and Actuators: describes the sensors and actuators used in the reference design.</li> <li>ECUs: describes the ECUs used in the reference design.</li> </ul> <p>The sensors requirements are different based on different PoV version.</p> <ul> <li> <p>Vision Pilot:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>One front-facing 4D RADAR.</li> </ul> </li> <li> <p>Vision Pilot Plus:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> </ul> </li> <li> <p>Vision Pilot Pro:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> <li>Four side camera: 8MP RGB camera with 100 degree horizontal FoV</li> <li>Sat-Nav 2D map</li> </ul> </li> <li> <p>Vision Drive:</p> <ul> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>One LWIR Camera: 60 degree horizontal FoV</li> <li>One SWIR Camera: 60 degree horizontal FoV</li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> <li>Four side camera: 8MP RGB camera with 100 degree horizontal FoV</li> <li>Sat-Nav 2D map</li> </ul> </li> </ul> <p>The locations of the sensors are shown in the figure below.</p> <p></p>"},{"location":"hardware-configuration/ECUs/","title":"ECUs","text":"<p>The page lists the ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <ul> <li>ARM-based ECUs: describes the hardware configuration for ARM-based ECUs.</li> <li>x86_64-based ECUs: describes the hardware configuration for x86_64-based ECUs</li> </ul>"},{"location":"hardware-configuration/ECUs/ARM/","title":"ARM-based ECUs","text":"<p>The page lists the ARM-based ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <p>Another page on Autoware Document lists the ECUs being for other use scenarios.</p> <p>(Candidates of the ECUs to be used by the PoV in the alphabetical order)</p>"},{"location":"hardware-configuration/ECUs/x86_64/","title":"X86-based ECUs","text":"<p>The page lists the X86-bsaed ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <p>Another page on Autoware Document lists the ECUs being for other use scenarios.</p> <p>(Candidates of the ECUs to be used by the LSA in the alphabetical order)</p>"},{"location":"hardware-configuration/Sensors-and-Actuators/","title":"Hardware Configuration","text":"<p>(To be completed)</p> <p>The page lists the sensors and actuators being tested for PoV, which can support the ODD of PoV.</p> <p>TODO: Add specific models Reference: https://autowarefoundation.github.io/LSA-reference-design-docs/main/hardware-configuration/Sensors-and-Actuators/</p> <ul> <li>Camera: Sony</li> <li>Radar: Bosch, Continental</li> <li>4D Radar: Bosch, Continental</li> <li>Infrared camera: Sony SenSWIR, Teledyne FLIR</li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot","title":"Vision Pilot","text":"<ul> <li>Camera:<ul> <li>1 forward front-facing 8MP RGB camera with 120 degree horizontal field-of-view.</li> </ul> </li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot-plus","title":"Vision Pilot Plus","text":"<ul> <li>Camera:<ul> <li>1 forward front-facing 8MP RGB camera with 120 degree horizontal field-of-view.</li> </ul> </li> <li>4D Radar:<ul> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> </ul> </li> <li>Radar<ul> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> </ul> </li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot-pro","title":"Vision Pilot Pro","text":"<ul> <li>Camera:<ul> <li>3x 8MP RGB camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> </ul> </li> <li>4D Radar:<ul> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> </ul> </li> <li>Radar<ul> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> <li>Standard 2D Navigational map with GPS</li> </ul> </li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-drive","title":"Vision Drive","text":"<ul> <li>Camera:<ul> <li>3x 8MP RGB camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> </ul> </li> <li>4D Radar:<ul> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> </ul> </li> <li>Radar<ul> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> <li>Standard 2D Navigational map with GPS</li> </ul> </li> <li>Infrared camera<ul> <li>1x forward front-facing 8MP Short Wave Infrared camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> <li>1x forward front-facing 8MP Long Wave Infrared camera with 120 degree horizontal field-of-view.</li> </ul> </li> </ul>"},{"location":"odd-definition/","title":"ODD Definition","text":"<p>Operation Domain Definition (ODD) describes the use scenarios for the vehicles to be designed and deployed. The deployed system should meet the ODD to be functionality completed. This section describes the ODD user scenarios for PoV.</p>"},{"location":"odd-definition/#vision-pilot","title":"Vision Pilot","text":"<p>Vision Pilot aims on Level 2+/2++ on the single lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. A human driver is required to monitor and supervise the system at all times.</p> <p>It will support the following features:</p> <ul> <li>Forward collision warning</li> <li>Lane departure warning</li> <li>Adaptive headlights</li> <li>Automatic Emergency braking</li> <li>Lane departure avoidance</li> <li>Intelligent speed assist</li> <li>Traffic aware autonomous cruise control</li> <li>Autonomous lane keep and road following</li> <li>Hands free autopilot</li> </ul> <p></p>"},{"location":"odd-definition/#vision-pilot-plus","title":"Vision Pilot - Plus","text":"<p>Vision Pilot - Plus aims on Level 3 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"odd-definition/#vision-pilot-pro","title":"Vision Pilot - Pro","text":"<p>Vision Pilot - Pro aims on Level 3 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"odd-definition/#vision-drive","title":"Vision Drive","text":"<p>Vision Drive aims on Level 4 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"software-configuration/","title":"Software Configuration","text":"<p>The software we deploy is VisionPilot</p> <p>AutoSeg is an AI Foundation Model which provides real-time visual scene perception for autonomous vehicles. It utilizes a single neural network backbone to extract diverse image features, a set of context blocks which focus the network's attention on key visual elements within input images, a set of feature necks which aggregate and fuse multi-scale image features, and multiple segmentation and detection heads which provide useful perceptual outputs for autonomous decision making. Overall, the network is split into three branches, a Depth Branch which calculates the per-pixel scene depth, a Scene Branch which focuses on per-pixel scene segmentation, and a Path Branch, which focuses on driving corridor detection through multiple means.</p> <p>AutoSeg is comprised of 6 neural network expert models which include:</p> <ul> <li>Scene3D: Metric and Relative depth estimation of scene elements.</li> <li>SceneSeg: Semantic segmentation of all foreground objects.</li> <li>DomainSeg: Semantic segmentation of roadwork zones and construction objects.</li> <li>EgoSpace: Semantic Segmentation of the drivable road surface.</li> <li>EgoPath: End-to-end drivable path prediction on roads, with and without lane markings.</li> <li>EgoLanes: Detection of lanes and road edges defining the driving corridor.</li> </ul> <p></p> <p>To run the VisionPilot, we need to install some prerequisites first.</p> <ul> <li>Installation</li> </ul> <p>Now that the Vision Pilot support two kinds of pipeline. You can follow the tutorial below to run the software.</p> <ul> <li>ROS 2</li> <li>Zenoh</li> </ul>"},{"location":"software-configuration/installation/","title":"Software installation","text":""},{"location":"software-configuration/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Ubuntu 22.04</li> <li>NVIDIA GPU</li> </ul>"},{"location":"software-configuration/installation/#prerequisites","title":"Prerequisites","text":"<p>Since VisionPilot requires AI packages, we need to install the following packages first.</p> <ul> <li>CUDA: Optional for GPU processing.</li> <li>OpenCV: For image and video processing.<ul> <li>Ubuntu: <code>sudo apt install libopencv-dev</code></li> </ul> </li> <li>ONNX Runtime: For model inference.<ul> <li>Download from the GitHub release</li> </ul> </li> <li>LibTorch: For tensor manipulation capabilities.<ul> <li>Download from the PyTorch website</li> </ul> </li> <li>TensorRT: Improve the inference performance.<ul> <li>Download from the website</li> <li>Follow the tutorial to install.</li> </ul> </li> </ul>"},{"location":"software-configuration/installation/#get-the-code-data","title":"Get the code &amp; data","text":"<ul> <li>Download the code</li> </ul> <pre><code>git clone https://github.com/autowarefoundation/autoware.privately-owned-vehicles.git\ncd autoware.privately-owned-vehicles/VisionPilot\n</code></pre> <ul> <li>Download the video: You can download any dashcam video on YouTube</li> </ul> <pre><code># Create a folder for the video and models\nmkdir -p data\ncd data\n# Put your video here, and name it as video.mp4\n</code></pre> <ul> <li>Download the models</li> </ul> <pre><code># Tool to download from Google Drive\npipx install gdown\n# SceneSeg\ngdown -O models/ 'https://docs.google.com/uc?export=download&amp;id=1l-dniunvYyFKvLD7k16Png3AsVTuMl9f'\n# Scene3D\ngdown -O models/ 'https://docs.google.com/uc?export=download&amp;id=19gMPt_1z4eujo4jm5XKuH-8eafh-wJC6'\n# DomainSeg\ngdown -O models/ 'https://docs.google.com/uc?export=download&amp;id=1zCworKw4aQ9_hDBkHfj1-sXitAAebl5Y'\n</code></pre>"},{"location":"software-configuration/ros2/","title":"ROS 2","text":""},{"location":"software-configuration/ros2/#dependencies","title":"Dependencies","text":"<p>We can follow the tutorial to install ROS 2 humble.</p> <pre><code># ROS2 packages\nsudo apt install ros-humble-cv-bridge ros-humble-image-transport\n# Build tools\nsudo apt install cmake build-essential\n</code></pre>"},{"location":"software-configuration/ros2/#build","title":"Build","text":"<ul> <li>Clean the project</li> </ul> <pre><code>rm -rf build install log\n</code></pre> <ul> <li>Build the code<ul> <li>Note that you need to update the path of the onnxruntime</li> </ul> </li> </ul> <pre><code>cd ROS2\nsource /opt/ros/humble/setup.bash\ncolcon build --symlink-install --packages-select sensors models visualization \\\n  --cmake-args \\\n  -DONNXRUNTIME_ROOTDIR=/path/to/onnxruntime \\\n  -DOpenCV_DIR=/usr/lib/x86_64-linux-gnu/cmake/opencv4 \\\n  -DCMAKE_BUILD_TYPE=Release\n</code></pre>"},{"location":"software-configuration/ros2/#usage","title":"Usage","text":"<pre><code>source install/setup.bash\n# SceneSeg\nros2 launch models run_pipeline.launch.py \\\n  pipeline:=scene_seg \\\n  video_path:=\"../data/video.mp4\"\n# DomainSeg\nros2 launch models run_pipeline.launch.py \\\n  pipeline:=domain_seg \\\n  video_path:=\"../data/video.mp4\"\n# Scene3D\nros2 launch models run_pipeline.launch.py \\\n  pipeline:=scene_3d \\\n  video_path:=\"../data/video.mp4\"\nros2 run image_tools showimage --ros-args -r image:=auto3d/scene_3d/viz # Visualize\n</code></pre> <p>Warning</p> <p>It will take sometimes to transform ONNX into TensorRT format first time you run the model.</p> <p></p>"},{"location":"software-configuration/zenoh/","title":"Zenoh","text":""},{"location":"software-configuration/zenoh/#dependencies","title":"Dependencies","text":"<p>We need install zenoh and some dependencies first.</p> <ul> <li>Zenoh C library: Required for the transportation.</li> <li>CLI11: Used for the command line interface.</li> <li>just: Simplify the command.</li> <li>parallel: Run commands in parallel</li> </ul> <pre><code># Setup the apt repositories\n## Eclipse Zenoh\ncurl -L https://download.eclipse.org/zenoh/debian-repo/zenoh-public-key | sudo gpg --dearmor --yes --output /etc/apt/keyrings/zenoh-public-key.gpg\necho \"deb [signed-by=/etc/apt/keyrings/zenoh-public-key.gpg] https://download.eclipse.org/zenoh/debian-repo/ /\" | sudo tee /etc/apt/sources.list.d/zenoh.list &gt; /dev/null\n## just\nwget -qO - 'https://proget.makedeb.org/debian-feeds/prebuilt-mpr.pub' | gpg --dearmor | sudo tee /usr/share/keyrings/prebuilt-mpr-archive-keyring.gpg 1&gt; /dev/null\necho \"deb [arch=all,$(dpkg --print-architecture) signed-by=/usr/share/keyrings/prebuilt-mpr-archive-keyring.gpg] https://proget.makedeb.org prebuilt-mpr $(lsb_release -cs)\" | sudo tee /etc/apt/sources.list.d/prebuilt-mpr.list\n## Install dependencies\nsudo apt update\nsudo apt install libzenohc-dev \\\n                 libcli11-dev \\\n                 just \\\n                 parallel\n</code></pre>"},{"location":"software-configuration/zenoh/#build","title":"Build","text":"<ul> <li>Clean the project</li> </ul> <pre><code>just clean\n</code></pre> <ul> <li>Build the code<ul> <li>Note that you need to export the path you install libraries first</li> </ul> </li> </ul> <pre><code>export LIBTORCH_INSTALL_ROOT=/path/to/libtorch/\nexport ONNXRUNTIME_ROOTDIR=/path/to/onnxruntime-linux-x64-gpu-1.22.0\ncd Zenoh\njust all\n</code></pre>"},{"location":"software-configuration/zenoh/#usage","title":"Usage","text":"<pre><code># Original video pub/sub\njust run_video_pubsub\n# SceneSeg\njust run_sceneseg\n# DomainSeg\njust run_domainseg\n# Scene3D\njust run_scene3d\n</code></pre> <p>Warning</p> <p>It will take sometimes to transform ONNX into TensorRT format first time you run the model.</p> <p></p>"},{"location":"system-configuration/","title":"System Configuration","text":"<p>The section describes the system configuration of the PoV vehicles. It shows the options to design your own PoV vehicles. The configuration consists of ECU selection, development approach, middleware candidates, etc.</p> <p>The system for PoV is still under software development (2025.09). This document will be updated when more information is available. This guideline provides the software architecture and software deployment. </p> <p>The system is organized as the following. VisionPilot separates core AI processing from middleware-specific implementations, enabling seamless deployment across different robotic frameworks. It supports ROS2 and Zenoh as the middleware layer. In the common layer, there are inference engineers, visualization engines, and sensor input processing module. </p> <p></p>"},{"location":"system-configuration/#version","title":"Version","text":"<p>There are four phases on the roadmap.</p> <p></p>"},{"location":"system-configuration/#hardware-selection","title":"Hardware Selection","text":"<p>The PoV software stack heavily relies on the inference models, which means high performant GPU is required.</p> <p>Here is the rough GPU requirements:</p> <ul> <li>Vision Pilot: 50-80 TOPS</li> <li>Vision Pilot Plus: 150 TOPS</li> <li>Vision Pilot Pro: 500 TOPS</li> <li>Vision Drive: 1000 TOPS</li> </ul> <p>NVIDIA GPU is well supported. AMD, Qualcomm and other vendors (e.g. NPU Halio-8) are on the roadmap.</p>"},{"location":"system-configuration/#development-approach","title":"Development Approach","text":"<ul> <li>Native Installation: Direct installation on hardware for maximum performance</li> <li>Containerized Development: Reproducible environments with easier team collaboration (on the roadmap)</li> </ul>"},{"location":"system-configuration/#middleware-choice","title":"Middleware Choice","text":"<p>Now the PoV supports two kinds of pipeline:</p> <ul> <li>ROS: Able to bridge with the existing Autoware system.</li> <li>Zenoh: High performant middleware.</li> </ul>"}]}