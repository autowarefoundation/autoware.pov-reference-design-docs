{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#about-reference-design-guideline-for-pov-vehicles","title":"About Reference Design Guideline for PoV Vehicles","text":"<p>This document serves a guideline to design and deploy a TRL-6 privately-owned vehicles based on Autoware. The readers can take this document as a starting point to select and configure the hardware and software components of the vehicles.</p> <p>The Autoware uses the end-to-end foundation model on the privately-owned vehicles, shown as below.</p> <p></p>"},{"location":"#reference-design-guideline-for-pov-vehicles-documentation-structure","title":"Reference Design Guideline for PoV Vehicles documentation structure","text":"<p>The document publishes the guidelines for Privately-owned Vehicles (PoV), using the following document structure shown below.</p> <p></p> <p>The document consists of four major components:</p> <ul> <li>ODD Definition defines the operation environment of the PoV.</li> <li>Hardware configuration describes the sensors and actuators used on the PoV. There is no reference physical chassis.</li> <li>Software configuration describes the process of deploying the software on ECUs.</li> <li>Evaluation and testing describes the process of evaluating and testing the software for the PoVs. The dataset and performance metrics are shown.</li> </ul> <p>There are fours phases on designing the software for PoV. The details of the roadmap are discussed on this page. Below summarizes the configuration and expected outcomes for each phase. This document provides the design guideline for Vision Pilot in Phase 1.</p> <ul> <li>Vision Pilot (Phase 1): using 1x forward front-facing RGB camera to enable near and long-range sensing to enable SAE Level 2+/2++ automation.</li> <li>Vision Pilot - Plus (Phase 2): using 1x forward front-facing RGB camera and 1x forward front-facing high resolution 4D radar, and 2x Blindspot monitoring automotive corner RADAR  to enable SAE Level 3 automation.</li> <li>Vision Pilot - Pro (Phase 3): using 3x forward front-facing RGB camera, 1x forward front-facing high resolution 4D radar, 2x Blindspot monitoring automotive corner RADAR, and standard 2D Navigational map with GPS to enable SAE Level 3 automation.</li> <li>Vision Drive (Phase 4): using 3x forward front-facing RGB camera, 1x forward front-facing high resolution 4D radar, 2x Blindspot monitoring automotive corner RADAR, and standard 2D Navigational map with GPS to enable SAE Level 4 automation.</li> </ul> <p></p> <p>For more details about the reference design WG, its goals and details of the Autoware Foundation working groups that oversees the project, refer to the Reference Design WG wiki</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>System Configuration</li> <li>ODD</li> <li>Hardware Configuration</li> <li>Software Configuration</li> <li>Evaluation and Testing</li> </ul>"},{"location":"evaluation-and-testing/","title":"Evaluation and Testing","text":"<p>(To be completed)</p>"},{"location":"evaluation-and-testing/#model-evaluation","title":"Model Evaluation","text":"<ul> <li>SceneSeg: https://github.com/autowarefoundation/autoware.privately-owned-vehicles/tree/main/SceneSeg#performance-results</li> <li>DomainSeg: https://github.com/autowarefoundation/autoware.privately-owned-vehicles/tree/main/DomainSeg#performance-results</li> </ul>"},{"location":"evaluation-and-testing/#system-evaluation","title":"System Evaluation","text":"<ul> <li>The overall performance for the whole pipeline.</li> </ul>"},{"location":"hardware-configuration/","title":"Hardware Configuration","text":"<p>(To be completed)</p> <p>This section describes the hardware configurations for PoV vehicles, including</p> <ul> <li>Sensors and Actuators: describes the sensors and actuators used in the reference design.</li> <li>ECUs: describes the ECUs used in the reference design.</li> </ul> <p>The sensors requirements are different based on different PoV version.</p> <ul> <li>Vision Pilot:</li> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li> <p>One front-facing 4D RADAR.</p> </li> <li> <p>Vision Pilot Plus:</p> </li> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>Two 4D RADAR: front and rear</li> <li> <p>Four blindspot Radar</p> </li> <li> <p>Vision Pilot Pro:</p> </li> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> <li>Four side camera: 8MP RGB camera with 100 degree horizontal FoV</li> <li> <p>Sat-Nav 2D map</p> </li> <li> <p>Vision Drive:</p> </li> <li>Two front-facing cameras:<ul> <li>a main camera: 8MP RGB camera with 120 degree horizontal FoV</li> <li>a long-range camera: 8MP RGB camera with 30 degree horizontal FoV</li> </ul> </li> <li>One LWIR Camera: 60 degree horizontal FoV</li> <li>One SWIR Camera: 60 degree horizontal FoV</li> <li>Two 4D RADAR: front and rear</li> <li>Four blindspot Radar</li> <li>Four side camera: 8MP RGB camera with 100 degree horizontal FoV</li> <li>Sat-Nav 2D map</li> </ul> <p>The locations of the sensors are shown in the figure below. </p>"},{"location":"hardware-configuration/ECUs/","title":"ECUs","text":"<p>The page lists the ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <ul> <li>ARM-based ECUs: describes the hardware configuration for ARM-based ECUs.</li> <li>x86_64-based ECUs: describes the hardware configuration for x86_64-based ECUs</li> </ul>"},{"location":"hardware-configuration/ECUs/ARM/","title":"ARM-based ECUs","text":"<p>The page lists the ARM-based ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <p>Another page on Autoware Document lists the ECUs being for other use scenarios.</p> <p>(Candidates of the ECUs to be used by the PoV in the alphabetical order)</p>"},{"location":"hardware-configuration/ECUs/x86_64/","title":"X86-based ECUs","text":"<p>The page lists the X86-bsaed ECUs being tested for PoV, which can meet the computation capacity requirement of PoV but does not overprovision the resource requirements.</p> <p>Another page on Autoware Document lists the ECUs being for other use scenarios.</p> <p>(Candidates of the ECUs to be used by the LSA in the alphabetical order)</p>"},{"location":"hardware-configuration/Sensors-and-Actuators/","title":"Hardware Configuration","text":"<p>(To be completed)</p> <p>The page lists the sensors and actuators being tested for PoV, which can support the ODD of PoV.</p> <p>TODO: Add specific models Reference: https://autowarefoundation.github.io/LSA-reference-design-docs/main/hardware-configuration/Sensors-and-Actuators/</p> <ul> <li>Camera: Sony</li> <li>Radar: Bosch, Continental</li> <li>4D Radar: Bosch, Continental</li> <li>Infrared camera: Sony SenSWIR, Teledyne FLIR</li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot","title":"Vision Pilot","text":"<ul> <li>Camera:</li> <li>1 forward front-facing 8MP RGB camera with 120 degree horizontal field-of-view.</li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot-plus","title":"Vision Pilot Plus","text":"<ul> <li>Camera:</li> <li>1 forward front-facing 8MP RGB camera with 120 degree horizontal field-of-view.</li> <li>4D Radar:</li> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> <li>Radar</li> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-pilot-pro","title":"Vision Pilot Pro","text":"<ul> <li>Camera:</li> <li>3x 8MP RGB camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> <li>4D Radar:</li> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> <li>Radar</li> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> <li>Standard 2D Navigational map with GPS</li> </ul>"},{"location":"hardware-configuration/Sensors-and-Actuators/#vision-drive","title":"Vision Drive","text":"<ul> <li>Camera:</li> <li>3x 8MP RGB camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> <li>4D Radar:</li> <li>1 forward front-facing high resolution (&lt; 0.5 degree angular resolution) long range (300 m)</li> <li>Radar</li> <li>2x Blindspot monitoring automotive corner radars (front-left, front-right)</li> <li>Standard 2D Navigational map with GPS</li> <li>Infrared camera</li> <li>1x forward front-facing 8MP Short Wave Infrared camera with 120 degree horizontal field-of-view. (forward-facing, front-right, front-left)</li> <li>1x forward front-facing 8MP Long Wave Infrared camera with 120 degree horizontal field-of-view.</li> </ul>"},{"location":"odd-definition/","title":"ODD Definition","text":"<p>Operation Domain Definition (ODD) describes the use scenarios for the vehicles to be designed and deployed. The deployed system should meet the ODD to be functionality completed. This section describes the ODD user scenarios for PoV.</p>"},{"location":"odd-definition/#vision-pilot","title":"Vision Pilot","text":"<p>Vision Pilot aims on Level 2+/2++ on the single lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. A human driver is required to monitor and supervise the system at all times.</p> <p>It will support the following features:</p> <ul> <li>Forward collision warning</li> <li>Lane departure warning</li> <li>Adaptive headlights</li> <li>Automatic Emergency braking</li> <li>Lane departure avoidance</li> <li>Intelligent speed assist</li> <li>Traffic aware autonomous cruise control</li> <li>Autonomous lane keep and road following</li> <li>Hands free autopilot</li> </ul> <p></p>"},{"location":"odd-definition/#vision-pilot-plus","title":"Vision Pilot - Plus","text":"<p>Vision Pilot - Plus aims on Level 3 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"odd-definition/#vision-pilot-pro","title":"Vision Pilot - Pro","text":"<p>Vision Pilot - Pro aims on Level 3 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"odd-definition/#vision-drive","title":"Vision Drive","text":"<p>Vision Drive aims on Level 4 on the multiple lane highway. It will operate on highway with clearly marked lane lines, under fair weather and visibility conditions within the full range of highway driving speeds (0 - 70 mph), and without the presence of construction zones or roadwork objects. The system can operate autonomous lane changes, and does not require a lead vehicle - however, the system does not handle highway merges or exits. Human supervision is not required as long as these conditions are met. Should these conditions be violated, the system will attempt to manoeuvre the vehicle to as safe a state as possible. </p> <p></p>"},{"location":"software-configuration/","title":"Software Configuration","text":"<p>The software we deploy is VisionPilot</p> <p>AutoSeg is an AI Foundation Model which provides real-time visual scene perception for autonomous vehicles. It utilizes a single neural network backbone to extract diverse image features, a set of context blocks which focus the network's attention on key visual elements within input images, a set of feature necks which aggregate and fuse multi-scale image features, and multiple segmentation and detection heads which provide useful perceptual outputs for autonomous decision making. Overall, the network is split into three branches, a Depth Branch which calculates the per-pixel scene depth, a Scene Branch which focuses on per-pixel scene segmentation, and a Path Branch, which focuses on driving corridor detection through multiple means.</p> <p>AutoSeg is comprised of 6 neural network export models which include:</p> <ul> <li>Scene3D: Metric and Relative depth estimation of scene elements. Detail</li> <li>SceneSeg: Semantic segmentation of all foreground objects. Detail</li> <li>DomainSeg: Semantic segmentation of roadwork zones and construction objects. Detail</li> <li>EgoSpace: Semantic Segmentation of the drivable road surface. Detail</li> <li>EgoPath: End-to-end drivable path prediction on roads, with and without lane markings. Detail</li> <li>EgoLanes: Detection of lanes and road edges defining the driving corridor. Detail</li> </ul> <p></p>"},{"location":"software-configuration/DomainSeg/","title":"DomainSeg","text":""},{"location":"software-configuration/DomainSeg/#overview","title":"Overview","text":"<p>DomainSeg addresses on the challenges on identifying roadwork scenes and construction objects, delivering robust safety perception across urban driving scenarios, highways and even unstrcutured roads. It is able to adapt to challenging weather conditions such as snow and low-light, and is robust to edge cases such as detection of traffic cones that have been knocked over by other cars. </p>"},{"location":"software-configuration/DomainSeg/#requirements","title":"Requirements","text":""},{"location":"software-configuration/DomainSeg/#demo","title":"Demo","text":"<p>Please see the Models folder to access the pre-trained network weights for DomainSeg as well as scripts for network training, inference and visualization of network predictions.</p> <p></p>"},{"location":"software-configuration/EgoLanes/","title":"EgoLanes","text":""},{"location":"software-configuration/EgoLanes/#overview","title":"Overview","text":"<p>Lane detection is an essential task in understanding the driving corridor for self-driving cars. Precise and accurate lane detection and road edges enables self-driving cars to remain centred in their driving corridor and track the curves of the road. To ensure safety, lane detection systems must be robust and generalisable across challenging weather conditions, lighting conditions and camera viewing/mounting angles. EgoLanes is a state-of-the-art neural network which detects all lanes and road edges on the road surface, and classifies the ego-lanes which a self-driving car should follow.</p>"},{"location":"software-configuration/EgoLanes/#requirements","title":"Requirements","text":"<pre><code>numpy==2.2.5\nopencv_contrib_python==4.10.0.84\nopencv_python==4.10.0.84\nopencv_python_headless==4.11.0.86\nPillow==11.3.0\n</code></pre>"},{"location":"software-configuration/EgoLanes/#demo","title":"Demo","text":""},{"location":"software-configuration/EgoPath/","title":"EgoPath","text":""},{"location":"software-configuration/EgoPath/#overview","title":"Overview","text":"<p>EgoPath is a neural network which processes raw camera image frames and directly predicts the driving path in an end-to-end manner, allowing for safe autonomous driving in challenging road conditions where lane detection alone is insufficient.</p>"},{"location":"software-configuration/EgoPath/#requirements","title":"Requirements","text":"<pre><code>numpy==2.2.5\nopencv_contrib_python==4.10.0.84\nopencv_python==4.10.0.84\nopencv_python_headless==4.11.0.86\nPillow==11.3.0\nscipy==1.15.2\ntqdm==4.66.5\n</code></pre>"},{"location":"software-configuration/EgoPath/#demo","title":"Demo","text":""},{"location":"software-configuration/EgoSpace/","title":"EgoLanes","text":""},{"location":"software-configuration/EgoSpace/#overview","title":"Overview","text":"<p>The EgoSpace network is part of the AutoSeg vision foundation model and is responsible for drivable road surface segmentation.</p>"},{"location":"software-configuration/EgoSpace/#requirements","title":"Requirements","text":""},{"location":"software-configuration/EgoSpace/#demo","title":"Demo","text":""},{"location":"software-configuration/Scene3D/","title":"Scene3D","text":""},{"location":"software-configuration/Scene3D/#overview","title":"Overview","text":"<p>Scene3D is able process monocular camera images to produce high resolution depth maps with sharp object boundaries, visible on the leaves of trees, thin structures such as poles, and on the edges of foreground objects - helping self-driving cars understand the dynamic driving scene in real-time. Scene3D enables important downstream perception tasks such as foreground obstacle detection, and is robust to changes in object appearance, size, shape and type, addressing 'long-tail' edge case scenarios. The current release of Scene3D estimates per-pixel relative depth, indicating which objects are nearer vs further away from the camera. Scene3D is part of the AutoSeg Foundation Model which forms the core of the vision-pipeline of the Autoware Autonomous Highway Pilot System.</p>"},{"location":"software-configuration/Scene3D/#requirements","title":"Requirements","text":"<pre><code>dgp==0.1.3\nmatplotlib==3.5.3\nModels==0.9.7\nnumpy==2.2.5\nopencv_contrib_python==4.10.0.84\nopencv_python==4.10.0.84\nopencv_python_headless==4.11.0.86\nPillow==11.3.0\n</code></pre>"},{"location":"software-configuration/Scene3D/#demo","title":"Demo","text":"<p>Please see the Models folder to access the pre-trained network weights for Scene3D as well as scripts for network training, inference and visualization of network predictions.</p>"},{"location":"software-configuration/SceneSeg/","title":"SceneSeg","text":""},{"location":"software-configuration/SceneSeg/#overview","title":"Overview","text":"<p>SceneSeg is a neural network that is able to segment all important foreground objects, irrespective of what that object is. SceneSeg is able to implicitly learn the visual features of foreground objects such as cars, buses, vans, pedestrians, cyclists, animals, rickshaws, trucks and other similar objects, even though it has not been explicitly trained to detect these object types. SceneSeg is also able to detect objects that are outside of its training data, such as tyres rolling down a highway, or a loose trailer. SceneSeg can also detect objects in unusual presentations that it hasn't seen during training. </p>"},{"location":"software-configuration/SceneSeg/#requirements","title":"Requirements","text":"<pre><code>Models==0.9.7\nPillow==11.3.0\n</code></pre>"},{"location":"software-configuration/SceneSeg/#demo","title":"Demo","text":"<p>Please click the video link to play - Video link</p> <p>Please see the Models folder to access the pre-trained network weights for SceneSeg as well as scripts for network training, inference and visualization of network predictions.</p>"},{"location":"software-configuration/installation/","title":"Software installation","text":"<p>TODO: Add the guideline how to install the prerequisites, e.g. CUDA, OpenCV, ROS, Zenoh...</p> <p>TODO: Add the detail how to build VisionPilot</p>"},{"location":"system-configuration/","title":"System Configuration","text":"<p>(To be completed)</p> <p>The section describes the system configuration of the PoV vehicles. It shows the options to design your own PoV vehicles. The configuration consists of ECU selection, development approach, middleware candidates, etc.</p>"},{"location":"system-configuration/#version","title":"Version","text":"<p>There are four phases on the roadmap.</p> <p></p>"},{"location":"system-configuration/#hardware-selection","title":"Hardware Selection","text":"<p>The PoV software stack heavily relies on the inference models, which means high performant GPU is required.</p> <p>Here is the rough GPU requirements:</p> <ul> <li>Vision Pilot: 50-80 TOPS</li> <li>Vision Pilot Plus: 150 TOPS</li> <li>Vision Pilot Pro: 500 TOPS</li> <li>Vision Drive: 1000 TOPS</li> </ul> <p>NVIDIA GPU is well supported. AMD, Qualcomm and other vendors (e.g. NPU Halio-8) are on the roadmap.</p>"},{"location":"system-configuration/#development-approach","title":"Development Approach","text":"<ul> <li>Native Installation: Direct installation on hardware for maximum performance</li> <li>Containerized Development: Reproducible environments with easier team collaboration (on the roadmap)</li> </ul>"},{"location":"system-configuration/#middleware-choice","title":"Middleware Choice","text":"<p>Now the PoV supports two kinds of pipeline:</p> <ul> <li>ROS: Able to bridge with the existing Autoware system.</li> <li>Zenoh: High performant middleware.</li> </ul>"}]}